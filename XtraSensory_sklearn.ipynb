{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XtraSensory_sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I95bWahrx_Dj"
      },
      "source": [
        "# Import the dependencies in use\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import zipfile\n",
        "\n",
        "def print_tripple_arr(a1, a2, a3):\n",
        "  for i in range(max([len(a1), len(a2), len(a3)])):\n",
        "    x1 = a1[i] if i < len(a1) else None\n",
        "    x2 = a2[i] if i < len(a2) else None\n",
        "    x3 = a3[i] if i < len(a3) else None\n",
        "    print(f\"{x1} {x2} {x3}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0FC6wFP66QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5db9b2a-1149-422f-890f-4bdec97436c3"
      },
      "source": [
        "# The datasets are stored in google drive as zip files\n",
        "# this cell unzips them into the working directory of the colab instance\n",
        "# goes from ~8Gb zipped to ~40GB unzipped\n",
        "\n",
        "drive_dir = \"/content/drive/MyDrive\"\n",
        "dset_zip = \"ExtraSensory.per_uuid_features_labels.zip\"\n",
        "\n",
        "def extract_xtrasensory_labels_to_pwd():\n",
        "  print(f\"extracting {drive_dir}/{dset_zip} to pwd\")\n",
        "  with zipfile.ZipFile(f\"{drive_dir}/{dset_zip}\", \"r\") as zip_f:\n",
        "    zip_f.extractall(path=\"./lbl\")\n",
        "\n",
        "lbl_glob_str = \"./lbl/*.features_labels.csv.gz\"\n",
        "glb = glob.glob(lbl_glob_str)\n",
        "if not glb:\n",
        "  extract_xtrasensory_labels_to_pwd()\n",
        "  glb = glob.glob(lbl_glob_str)\n",
        "\n",
        "glb.sort()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting /content/drive/MyDrive/ExtraSensory.per_uuid_features_labels.zip to pwd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgD0Wplxzd_O"
      },
      "source": [
        "def get_cols_of_interest():\n",
        "  df_cols = pd.read_csv(\"/content/lbl/00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\", compression='gzip').columns.to_list()\n",
        "  raw_ret = list(filter(lambda s: 'raw_acc:' in s, df_cols))\n",
        "  watch_ret = list(filter(lambda s: 'watch_acceleration:' in s, df_cols))\n",
        "  lbl_ret = list(filter(lambda s: 'label:' in s, df_cols))\n",
        "  return raw_ret, watch_ret, lbl_ret\n",
        "\n",
        "def get_dat_and_lbl_df(dat_cols, lbl_cols, glb):\n",
        "  dat_lst = []\n",
        "  lbl_lst =[]\n",
        "\n",
        "  for f_tgz in glb:\n",
        "    df = pd.read_csv(f_tgz, compression='gzip').get(dat_cols + lbl_cols)\n",
        "    df[lbl_cols] = df[lbl_cols].fillna(0)\n",
        "    df = df.dropna()\n",
        "    dat_lst.append(df[dat_cols])\n",
        "    lbl_lst.append(df[lbl_cols])\n",
        "  return dat_lst, lbl_lst\n",
        "\n",
        "def df_list_to_np_arr(df_lst):\n",
        "  ret_lst = []\n",
        "  for df in df_lst:\n",
        "    ret_lst.append(df.to_numpy())\n",
        "  return np.concatenate(ret_lst, axis=0)\n",
        "\n",
        "raw_coi, watch_coi, lbl_coi = get_cols_of_interest()\n",
        "# dat_df_lst, lbl_df_lst = get_dat_and_lbl_df(raw_coi + watch_coi, lbl_coi, glb[0:3])\n",
        "dat_df_lst, lbl_df_lst = get_dat_and_lbl_df(raw_coi + watch_coi, lbl_coi, glb)\n",
        "\n",
        "dat_arr = df_list_to_np_arr(dat_df_lst)\n",
        "lbl_arr = df_list_to_np_arr(lbl_df_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYf8lUuL_hHo"
      },
      "source": [
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "  def _balanced_acc_and_acc(y_true, y_pred):\n",
        "    '''\n",
        "    Ballanced accuracy = 1/2 * (tp/(tp+fn) + tn/(tn/fp))\n",
        "    '''\n",
        "    TP_c, TN_c, FP_c, FN_c = 0,0,0,0\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "      TP, TN, FP, FN = 0,0,0,0\n",
        "      trues = p[p == t]\n",
        "      falses = p[p != t]\n",
        "      TP = np.count_nonzero(trues)\n",
        "      TN = len(trues) - TP\n",
        "      FP = np.count_nonzero(falses)\n",
        "      FN = len(falses) - FP\n",
        "      TP_c += TP\n",
        "      TN_c += TN\n",
        "      FP_c += FP\n",
        "      FN_c += FN\n",
        "    specificity = TP_c/(TP_c+FN_c)\n",
        "    sensitivity = TN_c/(TN_c+FP_c)\n",
        "    return {\n",
        "        'ba': 0.5 * (specificity + sensitivity), \n",
        "        'a':(TP_c + TN_c)/(TP_c + TN_c + FN_c + FP_c), \n",
        "        'tp_rate': specificity, \n",
        "        'tn_rate': sensitivity,\n",
        "        }\n",
        "\n",
        "  pred = np.array(pred > threshold, dtype=float)\n",
        "  tmp_dict  = _balanced_acc_and_acc(y_true=target, y_pred=pred)\n",
        "  return {\n",
        "          # 'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
        "          # 'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
        "          'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro', zero_division=0),\n",
        "          # 'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
        "          # 'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
        "          'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro', zero_division=0),\n",
        "          # 'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
        "          # 'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
        "          'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples', zero_division=0),\n",
        "          'ba': tmp_dict['ba'],\n",
        "          'ss_a': accuracy_score(y_true=target, y_pred=pred),\n",
        "          'a': tmp_dict['a'],\n",
        "          'tp_rate': tmp_dict['tp_rate'],\n",
        "          'tn_rate': tmp_dict['tn_rate'],\n",
        "          }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPwMxTJ3vVfU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dat_arr, lbl_arr)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(dat_arr[0:20000], lbl_arr[0:20000])\n",
        "\n",
        "# cls = GaussianNB()\n",
        "# cls = DecisionTreeClassifier(max_depth=3)\n",
        "# cls = RandomForestClassifier(max_depth=3, verbose=1)\n",
        "# cls = AdaBoostClassifier(n_estimators=20)\n",
        "# multi_cls = MultiOutputClassifier(cls, n_jobs=-1)\n",
        "# multi_cls = multi_cls.fit(X_train, y_train)\n",
        "# y_pred = multi_cls.predict(X_test)\n",
        "# calculate_metrics(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpSUr0Ysxxao",
        "outputId": "2d3da6d5-5260-4bdc-d910-c8f2c1834e56"
      },
      "source": [
        "# with open(f\"{drive_dir}/multi_Adaboost20Estimator.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(multi_cls, f)\n",
        "\n",
        "with open(f\"{drive_dir}/multi_dtree.pkl\", \"rb\") as f:\n",
        "  multi_tree = pickle.load(f)\n",
        "\n",
        "with open(f\"{drive_dir}/multi_GaussNB.pkl\", \"rb\") as f:\n",
        "  multi_gauss_nb = pickle.load(f)\n",
        "\n",
        "with open(f\"{drive_dir}/multi_Adaboost10Estimator.pkl\", \"rb\") as f:\n",
        "  multi_ada_10e = pickle.load(f)\n",
        "\n",
        "with open(f\"{drive_dir}/multi_Adaboost20Estimator.pkl\", \"rb\") as f:\n",
        "  multi_ada_20e = pickle.load(f)\n",
        "\n",
        "print(f\"multi_tree:{calculate_metrics(multi_tree.predict(X_test), y_test)}\")\n",
        "print(f\"multi_gauss_nb:{calculate_metrics(multi_gauss_nb.predict(X_test), y_test)}\")\n",
        "print(f\"multi_ada_10e:{calculate_metrics(multi_ada_10e.predict(X_test), y_test)}\")\n",
        "print(f\"multi_ada_20e:{calculate_metrics(multi_ada_20e.predict(X_test), y_test)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_tree:{'micro/f1': 0.5222879955552883, 'macro/f1': 0.15201188265675494, 'samples/f1': 0.4125033282802891, 'ba': 0.7110329510902392, 'ss_a': 0.12266339869281045, 'a': 0.9465567730360118, 'tp_rate': 0.4393849593554698, 'tn_rate': 0.9826809428250086}\n",
            "multi_gauss_nb:{'micro/f1': 0.26902548274174726, 'macro/f1': 0.15071347024371246, 'samples/f1': 0.2897431381353074, 'ba': 0.7454941092876233, 'ss_a': 4.901960784313725e-05, 'a': 0.7200144175317186, 'tp_rate': 0.7748818248839933, 'tn_rate': 0.7161063936912532}\n",
            "multi_ada_10e:{'micro/f1': 0.5125710127236021, 'macro/f1': 0.11907784343018996, 'samples/f1': 0.41484303233077746, 'ba': 0.7025423346560074, 'ss_a': 0.10754901960784313, 'a': 0.9467807253620403, 'tp_rate': 0.42084315114368454, 'tn_rate': 0.98424151816833}\n",
            "multi_ada_20e:{'micro/f1': 0.5274041120879379, 'macro/f1': 0.1336248260161289, 'samples/f1': 0.42552021000060214, 'ba': 0.7086032135192797, 'ss_a': 0.12727124183006536, 'a': 0.9485377418941433, 'tp_rate': 0.4318680100804217, 'tn_rate': 0.9853384169581376}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_cols_of_interest()"
      ],
      "metadata": {
        "id": "QlPfgYos1nPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B1BBoYm9eI9"
      },
      "source": [
        "# df_cols = pd.read_csv(\"/content/lbl/00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\", compression='gzip').columns.to_list()\n",
        "# df_cols"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}